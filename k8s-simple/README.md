# üöÄ Kubernetes Manifests - Simple Version (Development)

B·ªô Kubernetes manifests ƒë∆°n gi·∫£n cho m√¥i tr∆∞·ªùng development v·ªõi gi·∫£i th√≠ch chi ti·∫øt t·ª´ng d√≤ng code.

## üìÅ C·∫•u tr√∫c th∆∞ m·ª•c

```
k8s-simple/
‚îú‚îÄ‚îÄ base/                    # Base manifests
‚îÇ   ‚îú‚îÄ‚îÄ namespace.yaml      # Namespace cho app
‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml      # Configuration kh√¥ng nh·∫°y c·∫£m
‚îÇ   ‚îú‚îÄ‚îÄ secret.yaml         # Credentials v√† sensitive data
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml     # Application deployment
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml        # Service ƒë·ªÉ expose pods
‚îÇ   ‚îú‚îÄ‚îÄ hpa.yaml            # Horizontal Pod Autoscaler
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml        # Ingress routing
‚îî‚îÄ‚îÄ README.md               # Documentation n√†y
```

## üìã Prerequisites

1. ‚úÖ AWS Account with permissions to create RDS, ElastiCache, and ALB
2. ‚úÖ Kubernetes cluster (EKS recommended for ALB support)
3. ‚úÖ `kubectl` installed and configured
4. ‚úÖ Docker for building images
5. ‚úÖ AWS Load Balancer Controller (for AWS ALB Ingress)
6. ‚úÖ Metrics Server (for HPA)
7. ‚úÖ `helm` (for installing AWS Load Balancer Controller)

## üéØ C√°c b∆∞·ªõc tri·ªÉn khai

### B∆∞·ªõc 1: T·∫°o AWS RDS MySQL

```bash
# T·∫°o RDS instance
aws rds create-db-instance \
  --db-instance-identifier dev-lbs-mysql \
  --db-instance-class db.t3.micro \
  --engine mysql \
  --engine-version 8.0.35 \
  --master-username admin \
  --master-user-password DevPass123! \
  --allocated-storage 20 \
  --vpc-security-group-ids sg-YOUR_SG_ID \
  --db-subnet-group-name your-subnet-group \
  --backup-retention-period 1 \
  --no-multi-az \
  --publicly-accessible

# Ch·ªù RDS ready, sau ƒë√≥ l·∫•y endpoint
aws rds describe-db-instances \
  --db-instance-identifier dev-lbs-mysql \
  --query 'DBInstances[0].Endpoint.Address' \
  --output text
```

### B∆∞·ªõc 2: T·∫°o AWS ElastiCache Redis

```bash
# T·∫°o ElastiCache cluster
aws elasticache create-cache-cluster \
  --cache-cluster-id dev-lbs-redis \
  --cache-node-type cache.t3.micro \
  --engine redis \
  --engine-version 7.0 \
  --num-cache-nodes 1 \
  --cache-subnet-group-name your-redis-subnet-group \
  --security-group-ids sg-YOUR_REDIS_SG

# Ch·ªù ElastiCache ready, sau ƒë√≥ l·∫•y endpoint
aws elasticache describe-cache-clusters \
  --cache-cluster-id dev-lbs-redis \
  --show-cache-node-info \
  --query 'CacheClusters[0].CacheNodes[0].Endpoint.Address' \
  --output text
```

### B∆∞·ªõc 3: C·∫≠p nh·∫≠t ConfigMap

M·ªü file `base/configmap.yaml` v√† c·∫≠p nh·∫≠t:

```yaml
# Line 25-26: C·∫≠p nh·∫≠t RDS endpoint
DB_HOST: "dev-lbs-mysql.xxxxxxxxx.us-east-1.rds.amazonaws.com"

# Line 36-37: C·∫≠p nh·∫≠t ElastiCache endpoint
REDIS_URL: "redis://dev-lbs-redis.xxxxxx.cache.amazonaws.com:6379"
```

### B∆∞·ªõc 4: C·∫≠p nh·∫≠t Secret

**Encode credentials:**
```bash
# Encode username
echo -n "admin" | base64
# Output: YWRtaW4=

# Encode password
echo -n "DevPass123!" | base64
# Output: RGV2UGFzczEyMyE=

# Encode JWT secret (t·∫°o random string)
echo -n "$(openssl rand -base64 32)" | base64
```

M·ªü file `base/secret.yaml` v√† c·∫≠p nh·∫≠t c√°c gi√° tr·ªã encoded.

### B∆∞·ªõc 5: Build v√† Push Docker Image

```bash
# Build image
cd ../../app-nestjs
docker build -t your-registry/location-based-service:dev-latest .

# Push image (n·∫øu d√πng registry)
docker push your-registry/location-based-service:dev-latest

# Ho·∫∑c load v√†o local cluster
# minikube image load your-registry/location-based-service:dev-latest
```

### B∆∞·ªõc 6: C·∫≠p nh·∫≠t Deployment Image

M·ªü file `base/deployment.yaml` v√† c·∫≠p nh·∫≠t line 96:

```yaml
image: your-registry/location-based-service:dev-latest
```

### B∆∞·ªõc 7: C√†i ƒë·∫∑t Prerequisites trong Cluster

#### C√†i ƒë·∫∑t Metrics Server (cho HPA)

```bash
# Install Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Ki·ªÉm tra Metrics Server
kubectl get deployment metrics-server -n kube-system

# Test metrics
kubectl top nodes
kubectl top pods -n lbs-dev
```

#### Install AWS Load Balancer Controller (for ALB)

```bash
# Prerequisites: EKS cluster with OIDC provider
# Create IAM OIDC provider for your EKS cluster
eksctl utils associate-iam-oidc-provider \
  --region us-east-1 \
  --cluster your-cluster-name \
  --approve

# Download IAM policy document
curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.2/docs/install/iam_policy.json

# Create IAM policy
aws iam create-policy \
  --policy-name AWSLoadBalancerControllerIAMPolicy \
  --policy-document file://iam_policy.json

# Create IAM service account
eksctl create iamserviceaccount \
  --cluster=your-cluster-name \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --attach-policy-arn=arn:aws:iam::ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve

# Install AWS Load Balancer Controller using Helm
helm repo add eks https://aws.github.io/eks-charts
helm repo update
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=your-cluster-name \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller

# Verify installation
kubectl get deployment -n kube-system aws-load-balancer-controller
kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
```

### B∆∞·ªõc 8: Deploy l√™n Kubernetes

```bash
# Apply t·∫•t c·∫£ manifests
cd ../k8s-simple
kubectl apply -f base/

# Ki·ªÉm tra deployment
kubectl get all -n lbs-dev
```

### B∆∞·ªõc 9: Configure DNS for AWS ALB

```bash
# Get ALB DNS name (after Ingress is created)
ALB_DNS=$(kubectl get ingress lbs-ingress -n lbs-dev -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "ALB DNS: $ALB_DNS"

# Example output:
# k8s-lbsdev-lbsingre-xxxxxxxxxx-123456789.us-east-1.elb.amazonaws.com

# Option 1: Test directly with ALB DNS
curl http://$ALB_DNS/health

# Option 2: Configure DNS CNAME record
# In your DNS provider (Route53, Cloudflare, etc.):
# Create CNAME: api.lbs-dev.local ‚Üí k8s-lbsdev-lbsingre-xxxxxxxxxx-123456789.us-east-1.elb.amazonaws.com

# Option 3: For local testing, add to /etc/hosts (not recommended for ALB)
# Get ALB IP (ALB uses multiple IPs, this is just for testing)
# nslookup $ALB_DNS
# echo "<ALB_IP> api.lbs-dev.local" | sudo tee -a /etc/hosts
```

## ‚úÖ Verify Deployment

```bash
# Ki·ªÉm tra pods
kubectl get pods -n lbs-dev

# Xem logs
kubectl logs -f deployment/lbs-app -n lbs-dev

# Ki·ªÉm tra service
kubectl get svc -n lbs-dev

# Ki·ªÉm tra HPA
kubectl get hpa -n lbs-dev

# Ki·ªÉm tra Ingress
kubectl get ingress -n lbs-dev
```

### Test Application

#### Option 1: Via AWS ALB (recommended)

```bash
# Get ALB DNS name
ALB_DNS=$(kubectl get ingress lbs-ingress -n lbs-dev -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

# Test with curl using ALB DNS directly
curl http://$ALB_DNS/health
curl http://$ALB_DNS/api/users

# Test with host header (if DNS not configured)
curl -H "Host: api.lbs-dev.local" http://$ALB_DNS/health

# If DNS is configured
curl http://api.lbs-dev.local/health
open http://api.lbs-dev.local
```

#### Option 2: Port Forward (n·∫øu Ingress ch∆∞a setup)

```bash
# Port forward service
kubectl port-forward svc/lbs-service 3000:80 -n lbs-dev

# Truy c·∫≠p
open http://localhost:3000
```

### Test HPA (Autoscaling)

```bash
# Xem current metrics
kubectl get hpa lbs-hpa -n lbs-dev

# Xem chi ti·∫øt HPA
kubectl describe hpa lbs-hpa -n lbs-dev

# Test autoscaling v·ªõi load
# Install hey tool: brew install hey (Mac) ho·∫∑c apt install hey (Linux)
hey -z 2m -c 50 http://api.lbs-dev.local/api/users

# Watch HPA trong terminal kh√°c
kubectl get hpa lbs-hpa -n lbs-dev --watch
```

## üìñ Gi·∫£i th√≠ch chi ti·∫øt c√°c files

### 1. `namespace.yaml` - Namespace

**M·ª•c ƒë√≠ch:** T·∫°o kh√¥ng gian t√™n ri√™ng cho app, gi√∫p t·ªï ch·ª©c v√† ph√¢n t√°ch resources.

**C√°c kh√°i ni·ªám:**
- `apiVersion: v1`: Version c·ªßa Kubernetes API cho resource n√†y
- `kind: Namespace`: Lo·∫°i resource
- `metadata.name`: T√™n namespace (lbs-dev)
- `labels`: Key-value pairs ƒë·ªÉ organize resources

### 2. `configmap.yaml` - Configuration Map

**M·ª•c ƒë√≠ch:** L∆∞u tr·ªØ configuration kh√¥ng nh·∫°y c·∫£m (endpoints, ports, settings).

**C√°c kh√°i ni·ªám:**
- `data`: Key-value pairs c·ªßa configuration
- ConfigMap ƒë∆∞·ª£c inject v√†o pods qua environment variables
- C√≥ th·ªÉ update ConfigMap m√† kh√¥ng c·∫ßn rebuild image

**C√°c config quan tr·ªçng:**
- `DB_HOST`: RDS endpoint
- `REDIS_URL`: ElastiCache endpoint
- `NODE_ENV`: M√¥i tr∆∞·ªùng runtime
- `RUN_SEEDER`: Ch·∫°y seeder hay kh√¥ng

### 3. `secret.yaml` - Secrets

**M·ª•c ƒë√≠ch:** L∆∞u tr·ªØ th√¥ng tin nh·∫°y c·∫£m (passwords, tokens, keys).

**C√°c kh√°i ni·ªám:**
- `type: Opaque`: Lo·∫°i secret ph·ªï bi·∫øn nh·∫•t
- D·ªØ li·ªáu ƒë∆∞·ª£c encode base64 (kh√¥ng ph·∫£i encryption)
- Kubernetes c√≥ th·ªÉ encrypt secrets at rest

**C√°c secret quan tr·ªçng:**
- `DB_USERNAME`, `DB_PASSWORD`: RDS credentials
- `JWT_SECRET_KEY`: Key ƒë·ªÉ sign JWT tokens
- `EMAIL_USER_NAME`, `EMAIL_PASSWORD`: Email credentials

### 4. `deployment.yaml` - Application Deployment

**M·ª•c ƒë√≠ch:** Qu·∫£n l√Ω vi·ªác deploy v√† scale application pods.

**C√°c ph·∫ßn quan tr·ªçng:**

#### a. Replicas
```yaml
replicas: 1
```
S·ªë l∆∞·ª£ng pods ch·∫°y song song. Dev: 1 pod l√† ƒë·ªß.

#### b. Selector
```yaml
selector:
  matchLabels:
    app: location-based-service
```
X√°c ƒë·ªãnh pods n√†o thu·ªôc v·ªÅ deployment n√†y.

#### c. Init Containers
```yaml
initContainers:
  - name: wait-for-rds
  - name: wait-for-redis
```
Ch·∫°y tr∆∞·ªõc main container ƒë·ªÉ:
- Check RDS MySQL connectivity
- Check ElastiCache Redis connectivity
- ƒê·∫£m b·∫£o dependencies s·∫µn s√†ng tr∆∞·ªõc khi app start

#### d. Main Container
```yaml
containers:
  - name: app
    image: your-registry/location-based-service:dev-latest
```
Container ch√≠nh ch·∫°y ·ª©ng d·ª•ng NestJS.

#### e. Environment Variables
```yaml
env:
  - name: DB_HOST
    valueFrom:
      configMapKeyRef:
        name: app-config
        key: DB_HOST
```
Inject config t·ª´ ConfigMap v√† Secret v√†o container.

#### f. Resource Limits
```yaml
resources:
  requests:
    memory: "256Mi"
    cpu: "100m"
  limits:
    memory: "512Mi"
    cpu: "500m"
```
- `requests`: T√†i nguy√™n t·ªëi thi·ªÉu c·∫ßn thi·∫øt
- `limits`: T√†i nguy√™n t·ªëi ƒëa c√≥ th·ªÉ s·ª≠ d·ª•ng

#### g. Health Checks
```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 3000
```
- `livenessProbe`: Check container c√≤n s·ªëng kh√¥ng (fail ‚Üí restart)
- `readinessProbe`: Check container s·∫µn s√†ng nh·∫≠n traffic kh√¥ng (fail ‚Üí remove kh·ªèi service)

### 5. `service.yaml` - Service

**M·ª•c ƒë√≠ch:** Expose pods ra ngo√†i v·ªõi stable IP/DNS.

**C√°c kh√°i ni·ªám:**
- `type: ClusterIP`: Service ch·ªâ accessible trong cluster
- `selector`: Ch·ªçn pods ƒë·ªÉ route traffic
- `port: 80`: Port external (trong cluster)
- `targetPort: 3000`: Port c·ªßa container

**Traffic flow:**
```
Request ‚Üí Service (port 80) ‚Üí Pod (port 3000) ‚Üí App
```

### 6. `hpa.yaml` - Horizontal Pod Autoscaler

**M·ª•c ƒë√≠ch:** T·ª± ƒë·ªông scale s·ªë l∆∞·ª£ng pods d·ª±a tr√™n CPU/Memory usage.

**C√°c kh√°i ni·ªám:**

#### a. Min/Max Replicas
```yaml
minReplicas: 1
maxReplicas: 3
```
- `minReplicas`: S·ªë pods t·ªëi thi·ªÉu (1 cho dev)
- `maxReplicas`: S·ªë pods t·ªëi ƒëa (3 cho dev)

#### b. Metrics
```yaml
metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```
- Scale up khi CPU usage > 70%
- Scale down khi CPU usage < 70%

#### c. Behavior
```yaml
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300
```
- `scaleDown`: Ch·ªù 5 ph√∫t tr∆∞·ªõc khi scale down (tr√°nh flapping)
- `scaleUp`: Scale up ngay l·∫≠p t·ª©c khi c·∫ßn

**C√°ch ho·∫°t ƒë·ªông:**
1. HPA check metrics m·ªói 15 gi√¢y
2. T√≠nh average CPU/Memory c·ªßa t·∫•t c·∫£ pods
3. So s√°nh v·ªõi target (70% CPU, 80% Memory)
4. N·∫øu > target: T·∫°o th√™m pods (scale up)
5. N·∫øu < target: X√≥a b·ªõt pods (scale down)

**Formula:**
```
desiredReplicas = ceil(currentReplicas * (currentMetric / targetMetric))
```

**V√≠ d·ª•:**
- Current: 1 pod, CPU = 85%
- Target: 70%
- Desired: ceil(1 * (85/70)) = ceil(1.21) = 2 pods
- HPA s·∫Ω t·∫°o th√™m 1 pod

### 7. `ingress.yaml` - Ingress

**M·ª•c ƒë√≠ch:** HTTP(S) routing t·ª´ b√™n ngo√†i v√†o services trong cluster.

**C√°c kh√°i ni·ªám:**

#### a. Ingress Controller
```yaml
ingressClassName: nginx
```
- NGINX Ingress Controller ph·∫£i ƒë∆∞·ª£c c√†i ƒë·∫∑t trong cluster
- Controller n√†y s·∫Ω t·∫°o LoadBalancer/NodePort ƒë·ªÉ nh·∫≠n traffic

#### b. Rules
```yaml
rules:
  - host: api.lbs-dev.local
    http:
      paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: lbs-service
              port:
                number: 80
```
- `host`: Domain name (DNS ph·∫£i point ƒë·∫øn Ingress Controller)
- `path`: URL path pattern
- `pathType: Prefix`: Match t·∫•t c·∫£ paths b·∫Øt ƒë·∫ßu v·ªõi /
- `backend`: Service ƒë·ªÉ forward traffic

#### c. Annotations
```yaml
annotations:
  nginx.ingress.kubernetes.io/ssl-redirect: "false"
  nginx.ingress.kubernetes.io/enable-cors: "true"
```
- `ssl-redirect`: T·ª± ƒë·ªông redirect HTTP ‚Üí HTTPS
- `enable-cors`: B·∫≠t CORS cho API
- `proxy-body-size`: Gi·ªõi h·∫°n request body size

**Traffic flow:**
```
Client ‚Üí DNS (api.lbs-dev.local) 
       ‚Üí Ingress Controller (NGINX) 
       ‚Üí Ingress Rules 
       ‚Üí Service (lbs-service:80) 
       ‚Üí Pods (lbs-app:3000) 
       ‚Üí App
```

**Setup:**
1. Install NGINX Ingress Controller
2. C·∫•u h√¨nh DNS ho·∫∑c /etc/hosts
3. Deploy Ingress manifest
4. Test: `curl http://api.lbs-dev.local`

## üîç Troubleshooting

### Pods kh√¥ng start?

```bash
# Xem chi ti·∫øt pod
kubectl describe pod POD_NAME -n lbs-dev

# Xem logs c·ªßa init container
kubectl logs POD_NAME -n lbs-dev -c wait-for-rds
kubectl logs POD_NAME -n lbs-dev -c wait-for-redis

# Xem logs c·ªßa main container
kubectl logs POD_NAME -n lbs-dev -c app
```

### Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c RDS/Redis?

1. Check security groups cho ph√©p traffic t·ª´ EKS
2. Check endpoints trong configmap ƒë√£ ƒë√∫ng ch∆∞a
3. Check credentials trong secret ƒë√£ ƒë√∫ng ch∆∞a

### Pod b·ªã CrashLoopBackOff?

```bash
# Xem logs ƒë·ªÉ bi·∫øt l√Ω do
kubectl logs POD_NAME -n lbs-dev --previous

# Ki·ªÉm tra events
kubectl get events -n lbs-dev --sort-by='.lastTimestamp'
```

## üßπ Cleanup

```bash
# X√≥a t·∫•t c·∫£ Kubernetes resources
kubectl delete -f base/

# X√≥a RDS
aws rds delete-db-instance \
  --db-instance-identifier dev-lbs-mysql \
  --skip-final-snapshot

# X√≥a ElastiCache
aws elasticache delete-cache-cluster \
  --cache-cluster-id dev-lbs-redis
```

## üìö H·ªçc th√™m

- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [Kubernetes Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)
- [AWS EKS Best Practices](https://aws.github.io/aws-eks-best-practices/)

---

**Ch√∫c b·∫°n deploy th√†nh c√¥ng! üéâ**
